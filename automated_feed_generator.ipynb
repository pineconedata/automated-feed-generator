{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b226ce46-3914-41cd-a261-7b8babf837fc",
   "metadata": {},
   "source": [
    "# Automate Your News Feed: How Web Scraping Powers a Python-Generated RSS Feed\n",
    "\n",
    "RSS feeds are a fantstic tool for reading the latest content from your favourite websites without cluttering up your email inbox or manually visiting each website. However, not every website owner publishes an official RSS feed anymore, making it difficult to access up-to-date content in one place. That's why today we'll be digging into how to generate your own personalized RSS feed using Python and web scraping. \n",
    "\n",
    "## What is an RSS feed?\n",
    "First off, an RSS (Really Simple Syndication) feed is a type of web feed that allows you to access updates to websites in a standardized, computer-readable format. Typically, a website owner will publish one RSS feed per website (or, for larger websites, one per category) that is updated regularly with new information. You can subscribe to multiple RSS feeds within an RSS feed reader (aka aggregator) - all you need to subscribe is the URL of the RSS feed. The RSS feed reader then displays an overview of the latest stories and information from all of your subscribed sites in one consolidated location. \n",
    "\n",
    "## Why use an RSS feed reader? \n",
    "If you haven't used an RSS feed reader before, you might not be familiar with the benefits they can offer over visiting a website directly or using a third-party news service. \n",
    "- Personalization - An RSS feed reader lets you personalize your news feed by subscribing only to the sources, topics, and categories that interest you. \n",
    "- Organization - You can easily organize your subscriptions in an RSS feed reader by creating folders, tagging articles, and prioritizing sources. \n",
    "- Improved Privacy - By using an RSS feed reader instead of visiting websites directly, you can protect your browsing data from being tracked by third-parties. \n",
    "- Fewer Distractions - Similar to the previous benefit, you can often bypass advertisements and intrusive popups that you might otherwise see on the original website. \n",
    "- Offline Accessibility - Many RSS feed readers offer the ability to save content for offline reading, allowing you to catch up on news or updates during periods of limited connectivity. \n",
    "\n",
    "## How do we get started? \n",
    "Now that we've covered the basics of RSS feeds and feed readers, let's dive in to how to generate an RSS feed for a website. In today's project, we'll use a website that does already publish an official RSS feed, but that will be useful for santiy-checking the end result. \n",
    "\n",
    "To get started, you'll need to have the following software installed on your system: \n",
    "\n",
    "- [Python](https://www.python.org/downloads/) (version 3.6 or higher)\n",
    "\n",
    "- [Selenium](https://pypi.org/project/selenium/) (Python library for web scraping)\n",
    "\n",
    "- [Firefox WebDriver](https://github.com/mozilla/geckodriver) (the browser we'll use for web scraping)\n",
    "\n",
    "- [feedgen](https://pypi.org/project/feedgen/) (Python library for RSS feed generation)\n",
    "\n",
    "Additional details about dependencies and version numbers can be found in the [`requirements.txt`](https://github.com/pineconedata/automated-feed-generator/blob/main/requirements.txt) file. \n",
    "\n",
    "*Note*: If you want to skip straight to implementation, you can follow the instructions in the [GitHub repo](https://github.com/pineconedata/automated-feed-generator) for this project. Otherwise, keep reading for step-by-step instructions and a breakdown of the code. \n",
    "\n",
    "## How will this process work?\n",
    "At a high level, this process will work by regularly running a Python script that will visit the desired website, scrapte the latest content, and export that content to an RSS feed file. For convenience, the configuration options have been separated from the Python script itself (so that it's easy to execute the same script for multiple websites), but you could easily combine these two files if you want. You can either run the Python script on your own computer (self-host) or use a third-party service (like AWS, GCP, Azure, etc.). This post will briefly cover configuring this script to run locally, so you won't need any accounts with any third-parties to run this process.\n",
    "\n",
    "# Configuration File\n",
    "Since the configuration options are stored in a separate file (in JSON format) from the Python script, let's take a look at the required configuration parameters first. In order to scrape a website to generate an RSS feed, we'll need to set basic parameters like the website URL and title, as well as more detailed parameters like how to identify the details for each individual item in the generated RSS feed. For example, if we look at a blog, these details would include the post title, post URL, post image, etc.\n",
    "\n",
    "In summary, the minimum parameters we need to specify are: \n",
    "\n",
    "- `website_url`: *Required*, URL of the website to scrape.\n",
    "- `website_title`: *Required*, Title for the RSS feed.\n",
    "- `website_description`: *Required*, Description for the RSS feed.\n",
    "- `posts_list_selector`: *Required*, CSS selector for the list of posts to include in the RSS feed. \n",
    "- `title_selector`: *Required*, CSS selector for the title of each element.\n",
    "- `link_selector`: *Required*, CSS selector for the link of each element.\n",
    "\n",
    "Here's an example of what those required configuration parameters would look like for NASA's Space Station blog: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "42a06000-50b0-4eba-85f2-c4c5a29be0b2",
   "metadata": {},
   "source": [
    "{\n",
    "  \"website_url\": \"https://blogs.nasa.gov/spacestation/\",\n",
    "  \"website_title\": \"NASA Space Station Blog\",\n",
    "  \"website_description\": \"The official blog of NASA's space station news.\",\n",
    "  \"posts_list_selector\": \"main#main > article\",\n",
    "  \"title_selector\": \"h2.entry-title\",\n",
    "  \"link_selector\": \"h2.entry-title > a\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb5abb0-2cde-40a0-b912-2deaef8e63ba",
   "metadata": {},
   "source": [
    "If we run the Python process with only these parameters, then we will get a bare-bones RSS feed that only populates the title and URL of each blog post from the website. However, you might want to generate a more robust RSS feed that includes the date, thumbnail image, and description of each blog post as well. To that end, there are several optional parameters that can be set in the configuration file: \n",
    "\n",
    "- `image_selector`: *Optional*, CSS selector for the image of each element.\n",
    "- `description_selector`: *Optional*, CSS selector for the description of each element.\n",
    "- `description_type`: *Optional*, Determines if the description should pull from the `text` or `innerHTML` attribute of the element located via the `description_selector`. Certain RSS readers can have issues with HTML description content, so the default is `text`. \n",
    "- `date_selector`: *Optional - if specified, `date_format` is required*, CSS selector for the date of each element.\n",
    "- `date_format`: *Optional - if specified, `date_selector` is required*, Date format of the date on the web page. This is used in conjunction with the `date_selector` to convert the date string to a datetime object using the [strptime](https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior) method.\n",
    "- `file_name`: *Optional*, Name of the output feed file. If not specified, the alphanumeric characters of `website_title` will be used instead.\n",
    "\n",
    "Here's an example of what the entire configuration JSON file would look like for NASA's Space Station blog: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "f838aa44-4d2f-4cff-b9a2-a83e67d15f52",
   "metadata": {},
   "source": [
    "{\n",
    "  \"website_url\": \"https://blogs.nasa.gov/spacestation/\",\n",
    "  \"website_title\": \"NASA Space Station Blog\",\n",
    "  \"website_description\": \"The official blog of NASA's space station news.\",\n",
    "  \"posts_list_selector\": \"main#main > article\",\n",
    "  \"title_selector\": \"h2.entry-title\",\n",
    "  \"link_selector\": \"h2.entry-title > a\",\n",
    "  \"image_selector\": \"div.entry-content > figure img\",\n",
    "  \"description_selector\": \"div.entry-content\",\n",
    "  \"description_type\": \"html\",\n",
    "  \"date_selector\": \"footer.entry-footer > span.posted-on > a > time\",\n",
    "  \"date_format\": \"%B %d, %Y\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1631d0f8-4d4a-43c8-8e7a-1e99ad453170",
   "metadata": {},
   "source": [
    "## CSS Selectors\n",
    "\n",
    "Once the script has opened the proper website URL, it will identify the list of posts to include in the RSS feed by using `posts_list_selector` (where `document.querySelectorAll(posts_list_selector)` should return the same number of HTML elements as the number of posts that should be included in your output RSS feed's content). Using the `document.querySelectorAll()` method is one of the quickest ways to identify your desired `posts_list_selector` value. MDN has additional details on the [`document.querySelectorAll()` method](https://developer.mozilla.org/en-US/docs/Web/API/Document/querySelectorAll) if you are not familiar with it.\n",
    "\n",
    "Then, the script will scrape the details of each post using the other selectors (`title_selector`, `link_selector`, `image_selector`, `description_selector`, and `date_selector`. The selectors for the post details (`title_selector`, `link_selector`, etc.) are sub-selectors of the `posts_list_selector`. For example, this sub-selector logic for the `title_selector` would be implemented in JavaScript as `document.querySelectorAll(posts_list_selector)...querySelector(title_selector)` (this script uses Python and Selenium, but the JS logic can be helpful for identifying the proper value of `title_selector`, etc. more quickly). MDN has additional details on the [`document.querySelector()` method](https://developer.mozilla.org/en-US/docs/Web/API/Document/querySelector) as well if you are not familiar with it.\n",
    "\n",
    "Writing precise, reliable CSS selectors can be challenging, but you can always start by right-clicking \"Inspect Element\" and then right-clicking \"Copy > CSS Selector\" on the desired HTML element. \n",
    "\n",
    "# Python Process\n",
    "Now that we've finished going through the configuration file, we can start going through the Python script.\n",
    "\n",
    "## Import Dependencies\n",
    "First, import all of the libraries that the script will depend on to function (aka dependencies). If you get any errors during this step, then you'll likely need to run `pip install` for any missing libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e41b6ea-0ec3-45a2-835b-2ced5ec73012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytz\n",
    "import time\n",
    "import json\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import FirefoxOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from feedgen.feed import FeedGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a92f0a-755c-4265-b183-8d14110ce229",
   "metadata": {},
   "source": [
    "## Import Configuration and Export Feed\n",
    "Now that we've covered all of the parameters in the configuration file, the first step of the Python script will be to import those paramters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec902f1-2f02-4676-b875-90c5fca1b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create an argument parser for command-line options\n",
    "    parser = argparse.ArgumentParser(description=\"Scrape a website and generate an RSS feed.\")\n",
    "\n",
    "    # Add and parse a  command-line argument for specifying the path to the configuration file \n",
    "    parser.add_argument('--config_file', required=True, help=\"Path to the configuration file\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Open and read the specified configuration file that contains the website and scraping parameters\n",
    "    with open(os.path.join(\"config\", args.config_file), 'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6b69bb-67de-4dc4-97a9-b49b8e2042d7",
   "metadata": {},
   "source": [
    "Once the configuration file is imported, we'll call the function that will scrape the website and generate the RSS feed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba0795-da2d-439b-97a1-db7aae709d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Scrape website and generate the RSS feed\n",
    "    rss_feed = scrape_and_generate_rss(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a557e-2366-4f96-bdb9-b44e7582b43b",
   "metadata": {},
   "source": [
    "We'll go over the details of that function next, but the last item in the main function is to export the RSS feed that is generated by that function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12fc09d-98fc-41a8-ab1e-1f5c7a1601b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Save the generated RSS feed to a file\n",
    "    if rss_feed:\n",
    "        file_name = config[\"file_name\"] if config.get(\"file_name\") else \"\".join(x for x in config[\"website_title\"] if x.isalnum())\n",
    "        file_path = os.path.join(\"feeds\", f'{file_name}.xml')\n",
    "        with open(file_path, 'wb') as rss_file:\n",
    "            rss_file.write(rss_feed)\n",
    "        print(f'RSS feed generated and saved as \"{file_path}\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240bfdd4-9f63-44d0-a858-64be3a81700d",
   "metadata": {},
   "source": [
    "## Scrape the Website\n",
    "Now we'll look at the details of the function that scrapes the website and generates the RSS feed. First, we'll want to parse the configuration options and configure the browser (which is the Firefox webdriver in this script). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31282015-b8fd-4f21-b08d-e3b3e1f01d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_generate_rss(config):\n",
    "    # Parse all of the settings from the configuration dictionary\n",
    "    website_url = config['website_url']\n",
    "    website_title = config['website_title']\n",
    "    website_description = config['website_description']\n",
    "    posts_list_selector = config['posts_list_selector']\n",
    "    title_selector = config.get('title_selector', None)\n",
    "    link_selector = config.get('link_selector', None)\n",
    "    image_selector = config.get('image_selector', None)\n",
    "    description_selector = config.get('description_selector', None)\n",
    "    description_type = config.get('description_type', None)\n",
    "    date_selector = config.get('date_selector', None)\n",
    "    date_format = config.get('date_format', None)\n",
    "\n",
    "    # Initialize a headless Firefox WebDriver for Selenium\n",
    "    opts = FirefoxOptions()\n",
    "    opts.add_argument(\"--headless\")\n",
    "    driver = webdriver.Firefox(options=opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5095269-0cca-4622-9c79-c32ba4d4539e",
   "metadata": {},
   "source": [
    "Once the webdriver is setup, we can navigate to the website's URL and create a (mostly empty) RSS feed object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1728c686-e26a-4c41-8c58-38adc46f3927",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Navigate to the specified website URL and wait for any dynamic content to load\n",
    "    driver.get(website_url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Create an RSS feed using FeedGenerator\n",
    "    fg = FeedGenerator()\n",
    "    fg.title(website_title)\n",
    "    fg.link(href=website_url, rel='alternate')\n",
    "    fg.description(website_description)\n",
    "    fg.ttl(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eca022-1b17-4de8-960e-60e06edde6dd",
   "metadata": {},
   "source": [
    "Once the feed object is created, we can begin populating it with the details of each post. Some of these details are optional, such as the thumbnail image and post date, and others are required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0effbbd3-3acf-4a9d-806f-c6412a698581",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T15:32:02.289688Z",
     "iopub.status.busy": "2023-09-27T15:32:02.289381Z",
     "iopub.status.idle": "2023-09-27T15:32:07.167335Z",
     "shell.execute_reply": "2023-09-27T15:32:07.166503Z",
     "shell.execute_reply.started": "2023-09-27T15:32:02.289674Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS feed generated and saved as \"NASASpaceStationBlog.xml\".\n"
     ]
    }
   ],
   "source": [
    "    # Find and iterate through the list of posts on the web page\n",
    "    posts_list = driver.find_elements(By.CSS_SELECTOR, posts_list_selector)\n",
    "    for post in posts_list:\n",
    "        # Create a new entry in the RSS feed for each post\n",
    "        fe = fg.add_entry()\n",
    "\n",
    "        # Extract information about each post (title, link, description, date, etc.) and add it to the feed entry\n",
    "        # Extract and set the post title\n",
    "        post_title = post.find_element(By.CSS_SELECTOR, title_selector).text\n",
    "        fe.title(post_title)\n",
    "\n",
    "        # Extract and set the post link\n",
    "        post_link = post.find_element(By.CSS_SELECTOR, link_selector).get_attribute('href')\n",
    "        fe.link(href=post_link)\n",
    "        fe.guid(post_link)\n",
    "\n",
    "        # Check if a description_selector is provided for extracting post descriptions\n",
    "        if description_selector:\n",
    "            # Check if a description_type is specified and it is set to 'html'. If so, extract the inner HTML content\n",
    "            if description_type and description_type == 'html':\n",
    "                post_description = post.find_element(By.CSS_SELECTOR, description_selector).get_attribute('innerHTML')\n",
    "            # If no description_type or it's not 'html', create a simple paragraph HTML structure for the post description\n",
    "            else:\n",
    "                post_description = f'<p>{post.find_element(By.CSS_SELECTOR, description_selector).text}</p>'\n",
    "\n",
    "        # Check if the image_selector is provided for extracting post image links\n",
    "        if image_selector:\n",
    "            # Check if there are any matches for the provided image_selector\n",
    "            image_elements = post.find_elements(By.CSS_SELECTOR, image_selector)\n",
    "            # Extract the link to the image and add it to the post description\n",
    "            if image_elements:\n",
    "                image_link = image_elements[0].get_attribute('src')\n",
    "                post_description += f'<img src=\"{image_link}\" alt=\"{post_title}\">'\n",
    "        fe.description(post_description)\n",
    "\n",
    "        # Check if date parameeters are provided for extracting the post date\n",
    "        if date_selector and date_format:\n",
    "            # Extract and format the post date \n",
    "            post_date = post.find_element(By.CSS_SELECTOR, date_selector).text\n",
    "            post_date = datetime.strptime(post_date, date_format).replace(tzinfo=pytz.utc)\n",
    "            fe.pubDate(post_date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc1a52a-134d-4637-825b-320f247db653",
   "metadata": {},
   "source": [
    "After iterating through all of the posts, we can pretty-print and return the final RSS feed, as well as close the webdriver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f17dfd3-daab-42c8-8304-a7f6d04c9f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Generate the RSS feed and return it as a string\n",
    "    rss_feed = fg.rss_str(pretty=True)\n",
    "\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "    return rss_feed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a1308-8763-4e9d-8855-b1049a720d22",
   "metadata": {},
   "source": [
    "That's the entire Python script! It's a fairly simple process, and in the next section we'll go over how to run this script from the command line. \n",
    "\n",
    "# Running the Process\n",
    "In order to run the process, there should be a particular folder structure for the files (illustrated below). "
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0ef654b-17dc-449f-88fb-3f60c53716c6",
   "metadata": {},
   "source": [
    "-[parent folder]\n",
    "--automated_feed_generator.py (the Python script)\n",
    "--[config] (a sub-folder that holds the configuration files)\n",
    "---NASASpaceStationBlog.json (the configuration file)\n",
    "--[feeds] (a sub-folder that holds the RSS feed files, once generated)\n",
    "---NASASpaceStationBlog.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1a884f-02e6-4b52-9d37-3c2d83148b21",
   "metadata": {},
   "source": [
    "Once you have this folder structure setup (although the `feeds` directory will be empty right now), you can run the script from the same folder as the `automated_feed_generator.py` file with the configuration file as a command-line argument. Here's an example: \n",
    "\n",
    "```shell\n",
    "python3 automated_feed_generator.py --config_file 'NASASpaceStationBlog.json'\n",
    "```\n",
    "Once you run this command, the script will parse the configuration file, scrape the website, generate an RSS feed, and save it as an XML file in the `feeds` directory. If the output filename is not specified in the configuration file, then the filename is derived from the title of the website (with any non-alphanumeric characters removed).\n",
    "\n",
    "# Scheduling\n",
    "\n",
    "The entire process up to this point will only generate the RSS feed once. To keep your feed(s) up-to-date, you can schedule this Python script to run regularly. There are a variety of ways to do this, but the simplest example is setting it up as a cron job. You can use any cron job manager you like, but the example provided below works with [crontab](https://man7.org/linux/man-pages/man5/crontab.5.html).\n",
    "\n",
    "1. Open your crontab configuration by running `crontab -e` as usual. \n",
    "\n",
    "2. Add a cron job entry to schedule the script at your [desired frequency](https://crontab.guru). For example, to run the script every day at 2:00 AM, you can add the following line:\n",
    "\n",
    "```bash\n",
    "0 2 * * * python3 ~/path/to/dir/automated-feed-generator/automated_feed_generator.py --config_file 'NASASpaceStationBlog.json'\n",
    "```\n",
    "\n",
    "- Make sure to replace `~/path/to/dir/automated-feed-generator/` with the actual directory where your Python script (`automated_feed_generator.py`) is located. \n",
    "\n",
    "- Add a separate line to your crontab file for each job that you want to schedule (typically one per configuration file).\n",
    "\n",
    "3. Alternatively, you might want to run this Python script for all of the configuration files in the `config` directory at once and add only one line to your crontab configuration. In that case, you can move the Python script into a Shell script, like this: \n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "cd ~/path/to/dir/automated-feed-generator\n",
    "\n",
    "# Directory containing configuration files\n",
    "configs_dir=\"config\"\n",
    "\n",
    "# Iterate over each file in the configs directory\n",
    "for config_file in \"$configs_dir\"/*.json; do\n",
    "    if [ -e \"$config_file\" ]; then\n",
    "        # Extract the file name without the directory path\n",
    "        config_file_name=$(basename \"$config_file\")\n",
    "\n",
    "        echo \"Processing $config_file_name...\"\n",
    "        python3 automated_feed_generator.py --config_file \"$config_file_name\"\n",
    "    fi\n",
    "done\n",
    "```\n",
    "\n",
    "- Just like above, make sure to replace `~/path/to/dir/automated-feed-generator` with the actual directory where your Python script (`automated_feed_generator.py`) is located. \n",
    "\n",
    "Then, you can add this script as a single cron job that will update all of the feeds at once. Here's an example of scheduling this script to run daily: \n",
    "\n",
    "```bash\n",
    "@daily ~/path/to/dir/automated_feed_generator.sh\n",
    "```\n",
    "\n",
    "Now each `config_file.json` in the `config` directory will be passed to the `automated_feed_gneerator.py` script and will output a resulting file in the `feeds` directory. All that's left is to host your `feeds` directory somewhere that a RSS feed reader can pull from.\n",
    "\n",
    "# Limitations\n",
    "\n",
    "Before we wrap up, there are a few limitations to this Python process. There are workarounds for these limitations, but they are not covered in today's project. \n",
    "\n",
    "- **iFrames**: This script does not out-of-the-box support selectors that are within [iframes](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/iframe).\n",
    "    - However, there could be ways around this limitation, depending on the site structure and iframe details. A first step might be to fork this repo and modify the `posts_list = driver.find_elements(By.CSS_SELECTOR, posts_list_selector)` logic to something like `posts_list = driver.find_element(By.CSS_SELECTOR, iframe_selector).find_elements(By.CSS_SELECTOR, posts_list_selector)`. Note that this example logic is untested and might not work in all scenarios. \n",
    "- **Shadow DOMs**: This script does not out-of-the-box support selectors that are within [shadow DOMs](https://developer.mozilla.org/en-US/docs/Web/API/Web_components/Using_shadow_DOM).\n",
    "    - Similar to iframes, there could be ways around this limitation. One possible solution might involve selecting the shadow DOM element and then selecting the posts_list (in JavaScript, this would look something like `document.querySelector(shadow_dom_selector).shadowRoot.querySelectorAll(posts_list_selector)`). \n",
    "- **Blocking**: This script is meant to be run at a low-volume (once per day) from a personal machine that has access to the website that you are scraping. This is not intended to be used for any malicious purposes, and, as such, no steps have been taken to ensure that the website owner does not block or tarpit your traffic.\n",
    "    - Your traffic typically will not get blocked from running this script once per day. However, website owners have different policies and some might be more aggressive about blocking traffic (such as blocking all Linux+Firefox traffic). If you are concerned about getting blocked, then there is plenty of additional logic that could be added to this script to mitigate those risks.\n",
    "\n",
    "# Wrap up\n",
    "We've finished thoroughly going over how to write, run, and schedule a Python script that will scrape a website and generate an RSS feed. If you found this information helpful, please give it a like, share, or fork the [GitHub repo](https://github.com/pineconedata/automated-feed-generator). If you have any questions or suggestions, feel free to [contact me](/workwithme) or open a pull request! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43328f0-e504-4ad6-a112-903e834d4a89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
