{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0effbbd3-3acf-4a9d-806f-c6412a698581",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T15:32:02.289688Z",
     "iopub.status.busy": "2023-09-27T15:32:02.289381Z",
     "iopub.status.idle": "2023-09-27T15:32:07.167335Z",
     "shell.execute_reply": "2023-09-27T15:32:07.166503Z",
     "shell.execute_reply.started": "2023-09-27T15:32:02.289674Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS feed generated and saved as \"NASASpaceStationBlog.xml\".\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pytz\n",
    "import time\n",
    "import json\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import FirefoxOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from feedgen.feed import FeedGenerator\n",
    "\n",
    "\n",
    "def scrape_and_generate_rss(config):\n",
    "    # Parse all of the settings from the configuration dictionary\n",
    "    website_url = config['website_url']\n",
    "    website_title = config['website_title']\n",
    "    website_description = config['website_description']\n",
    "    elements_selector = config['elements_selector']\n",
    "    title_selector = config['title_selector']\n",
    "    link_selector = config['link_selector']\n",
    "    image_selector = config['image_selector']\n",
    "    description_selector = config['description_selector']\n",
    "    date_selector = config['date_selector']\n",
    "    date_format = config['date_format']\n",
    "\n",
    "    # Initialize a headless Firefox WebDriver for Selenium\n",
    "    opts = FirefoxOptions()\n",
    "    opts.add_argument(\"--headless\")\n",
    "    driver = webdriver.Firefox(options=opts)\n",
    "\n",
    "    # Navigate to the specified website URL and wait for any dynamic content to load\n",
    "    driver.get(website_url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Create an RSS feed using FeedGenerator\n",
    "    fg = FeedGenerator()\n",
    "    fg.title(website_title)\n",
    "    fg.link(href=website_url, rel='alternate')\n",
    "    fg.description(website_description)\n",
    "\n",
    "    # Find and iterate through the elements on the web page\n",
    "    elements = driver.find_elements(By.CSS_SELECTOR, elements_selector)\n",
    "    for element in elements:\n",
    "        # Extract information about each item (title, link, description, date, etc.)\n",
    "        item_title = element.find_element(By.CSS_SELECTOR, title_selector).text\n",
    "        item_link = element.find_element(By.CSS_SELECTOR, link_selector).get_attribute('href')\n",
    "        image_link = element.find_element(By.CSS_SELECTOR, image_selector).get_attribute('src')\n",
    "        item_description = f'<p>{element.find_element(By.CSS_SELECTOR, description_selector).text}</p>'\n",
    "        item_description += f'<img src=\"{image_link}\" alt=\"{item_title}\">'\n",
    "        item_date = element.find_element(By.CSS_SELECTOR, date_selector).text\n",
    "        item_date = datetime.strptime(item_date, date_format).replace(tzinfo=pytz.utc)\n",
    "\n",
    "        # Create a new entry for the item in the RSS feed\n",
    "        fe = fg.add_entry()\n",
    "        fe.title(item_title)\n",
    "        fe.link(href=item_link)\n",
    "        fe.guid(item_link)\n",
    "        fe.description(item_description)\n",
    "        fe.pubDate(item_date)\n",
    "\n",
    "    # Generate the RSS feed and return it as a string\n",
    "    rss_feed = fg.rss_str(pretty=True)\n",
    "\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "    return rss_feed\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an argument parser for command-line options\n",
    "    parser = argparse.ArgumentParser(description=\"Scrape a website and generate an RSS feed.\")\n",
    "\n",
    "    # Add and parse a  command-line argument for specifying the path to the configuration file \n",
    "    parser.add_argument('--config_file', required=True, help=\"Path to the configuration file\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Open and read the specified configuration file that contains the website and scraping parameters\n",
    "    with open(args.config_file, 'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "\n",
    "    # Scrape website and generate the RSS feed\n",
    "    rss_feed = scrape_and_generate_rss(config)\n",
    "\n",
    "    # Save the generated RSS feed to a file\n",
    "    if rss_feed:\n",
    "        file_name = f'{\"\".join(x for x in config[\"website_title\"] if x.isalnum())}.xml'\n",
    "        with open(f'{os.path.join(\"feeds\", file_name)}', 'wb') as rss_file:\n",
    "            rss_file.write(rss_feed)\n",
    "        print(f'RSS feed generated and saved as \"{file_name}\".')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
